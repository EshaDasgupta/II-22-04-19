{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "import keras.models\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import _Merge,Multiply\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class QLayer(_Merge):\n",
    "    '''Q Layer that merges an advantage and value layer'''\n",
    "    def _merge_function(self, inputs):\n",
    "        '''Assume that the inputs come in as [value, advantage]'''\n",
    "        output = inputs[0] + (inputs[1] - K.mean(inputs[1], axis=1, keepdims=True))\n",
    "        return output\n",
    "    \n",
    "class DQNSolver():#QNetwork():\n",
    "    \n",
    "    GAMMA = 0.95\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    MEMORY_SIZE = 1000000\n",
    "    BATCH_SIZE = 20\n",
    "\n",
    "    EXPLORATION_MAX = 1.0\n",
    "    EXPLORATION_MIN = 0.01\n",
    "    EXPLORATION_DECAY = 0.995\n",
    "\n",
    "    def __init__(self, observation_space, action_space,height,width):\n",
    "        \n",
    "        self.exploration_rate = self.EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=self.MEMORY_SIZE)\n",
    "        \n",
    "        self.inputs=Input(shape=(height,width,1))\n",
    "        self.actions = Input(shape=(1,), dtype='int32')\n",
    "        self.actions_onehot = Lambda(K.one_hot, arguments={'num_classes':self.action_space}, output_shape=(None, self.action_space))(self.actions)\n",
    "        \n",
    "        x = Conv2D(filters=32, kernel_size=[2,2], strides=[4,4], input_shape=(height,width,1),activation=\"elu\")(self.inputs)\n",
    "        x = Conv2D(filters=64, kernel_size=[2,2],strides=[2,2],activation=\"elu\")(x)\n",
    "        x = Conv2D(filters=512, kernel_size=[1,1],strides=[1,1],activation=\"elu\")(x)\n",
    "                \n",
    "        #Splice outputs of last conv layer using lambda layer\n",
    "        x_value = Lambda(lambda x: x[:,:,:,:512//2])(x)\n",
    "        x_advantage = Lambda(lambda x: x[:,:,:,512//2:])(x)\n",
    "        \n",
    "        #Process spliced data stream into value and advantage function\n",
    "        value = Dense(self.action_space, activation=\"linear\")(x_value) \n",
    "        advantage = Dense(self.action_space, activation=\"linear\")(x_advantage)\n",
    "        \n",
    "        #Recombine value and advantage layers into Q layer\n",
    "        q = QLayer()([value, advantage])\n",
    "        self.q_out = Multiply()([q, self.actions_onehot])\n",
    "        self.q_out = Lambda(lambda x: K.cumsum(x, axis=3), output_shape=(1,))(self.q_out)\n",
    "        \n",
    "        #need to figure out how to represent actions within training\n",
    "        self.model = Model(inputs=[self.inputs, self.actions], outputs=[q, self.q_out]) \n",
    "        self.model.compile(optimizer=Adam(lr=self.LEARNING_RATE), loss=\"mean_squared_error\")\n",
    "        \n",
    "        self.target_model=self.copy_model()\n",
    "        \n",
    "        \"\"\"self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.LEARNING_RATE))\n",
    "    \"\"\"\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #experience=[state, action, reward, next_state, done]\n",
    "        #self.memory.store(experience)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def copy_model(self):\n",
    "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
    "        self.model.save('tmp_model')\n",
    "        return keras.models.load_model('tmp_model',custom_objects={'QLayer':QLayer, 'tf':tf})\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict([np.array(state).reshape((1,np.array(state).shape[0],np.array(state).shape[1],np.array(state).shape[2])),np.asarray([1])])#state)\n",
    "        #print(\"Q_values \",q_values[0],\"Max \",np.argmax(q_values[0]))\n",
    "        #print(\"Q Values\",q_values[0][0][0][0],len(q_values[0][0][0][0]))\n",
    "        #if(len(q_values[0][0][0][0])>5):\n",
    "        #    print(\"Problem\",q_values[0][0][0][0])\n",
    "        #print(\"returning\",np.argmax(q_values[0][0][0]))\n",
    "        return min(np.argmax(q_values[0][0][0]),4)\n",
    "\n",
    "    def experience_replay(self):        \n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            #print(\"In replay return\")\n",
    "            return\n",
    "        #print(\"In replay full\")\n",
    "        batch = random.sample(self.memory, self.BATCH_SIZE)\n",
    "        #print(\"1\")\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward            \n",
    "            #print(\"2\")\n",
    "            if not terminal:                \n",
    "                #print(np.array(state_next).shape)\n",
    "                statenext_dash=np.array(state_next).reshape((1,np.array(state_next).shape[0],np.array(state_next).shape[1],np.array(state_next).shape[2]))\n",
    "                \n",
    "                #print(\"3\",self.model.predict([statenext_dash,np.asarray([1])])[0],np.amax(self.model.predict([statenext_dash,np.asarray([1])])[0]))\n",
    "                a_dash_val=self.model.predict([statenext_dash,np.asarray([1])])[0]\n",
    "                a_dash=np.argmax(a_dash_val[0][0][0])\n",
    "                #print(\"a_dash\",a_dash)\n",
    "                q_update = (reward + self.GAMMA * (self.target_model.predict([statenext_dash,np.array([1])])[0])[0][0][0][a_dash])\n",
    "            #print(\"4\")\n",
    "            q,q_values = self.target_model.predict([np.array(state).reshape((1,np.array(state).shape[0],np.array(state).shape[1],np.array(state).shape[2])),np.asarray([1])])\n",
    "            #print(\"5\",q_values)\n",
    "            q_values[0][0][0][action] = q_update\n",
    "            #print(\"6\")\n",
    "            fit_in= [np.array(state).reshape((1,np.array(state).shape[0],np.array(state).shape[1],np.array(state).shape[2])),np.asarray([1])]\n",
    "            #q_values=q_values.reshape((q_values.shape[2],q_values.shape[3]))\n",
    "            #q=q.reshape((q.shape[2],q.shape[3]))\n",
    "            fit_out=[q_values,np.array([1])]#fit_out=[q, q_values[0]]#.reshape((q_values.shape[2],q_values.shape[3]))]\n",
    "            #print(\"Fit_in\",fit_in)\n",
    "            #print(\"Fit_out\",fit_out)\n",
    "            #print(\"in\",fit_in[0].shape, fit_in[1].shape)\n",
    "            #print(\"out\",fit_out[0].shape, fit_out[1].shape) #q.reshape((q.shape[2],q.shape[3]))\n",
    "            self.model.fit(fit_in, fit_out, verbose=0)\n",
    "            #print(\"7\")\n",
    "        #print(\"8\")    \n",
    "        self.exploration_rate *= self.EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(self.EXPLORATION_MIN, self.exploration_rate)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        #print(\"9\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 5)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s=np.asarray([[[[1,2,3,4,5]]]])\n",
    "print(s.shape)\n",
    "print(s.reshape((5,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
